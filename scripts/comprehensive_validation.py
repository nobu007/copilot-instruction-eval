#!/usr/bin/env python3
"""
Comprehensive Validation Script - VSCode Copilot Automation System
„Ç∑„Çπ„ÉÜ„É†ÂÖ®‰Ωì„ÅÆÂåÖÊã¨ÁöÑÊ§úË®º„Éª„ÉÜ„Çπ„ÉàÂÆüË°å„Çπ„ÇØ„É™„Éó„Éà

Usage:
    python3 scripts/comprehensive_validation.py [--quick] [--report-only]
"""
import json
import os
import time
import uuid
import logging
import argparse
import sqlite3
import subprocess
from datetime import datetime
from typing import List, Dict, Any, Optional

# „É≠„ÇÆ„É≥„Ç∞Ë®≠ÂÆö
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class ComprehensiveValidator:
    def __init__(self):
        self.base_dir = "/tmp/copilot-evaluation"
        self.requests_dir = f"{self.base_dir}/requests"
        self.responses_dir = f"{self.base_dir}/responses"
        self.db_path = "validation_results.db"
        
        # „ÉÜ„Çπ„Éà„Ç±„Éº„Çπ„Ç´„ÉÜ„Ç¥„É™
        self.test_categories = {
            "connectivity": "Basic connectivity and system responsiveness",
            "functionality": "Core Copilot functionality",
            "error_handling": "Error conditions and edge cases",
            "performance": "Response time and throughput",
            "reliability": "Consistency and failure recovery"
        }
        
        # ÂåÖÊã¨ÁöÑ„ÉÜ„Çπ„Éà„Ç±„Éº„Çπ
        self.test_cases = [
            # Êé•Á∂öÊÄß„ÉÜ„Çπ„Éà
            {
                "id": "conn_ping_basic",
                "category": "connectivity",
                "command": "ping",
                "params": {},
                "expected_status": "success",
                "timeout": 10,
                "description": "Basic ping connectivity test"
            },
            {
                "id": "conn_current_state",
                "category": "connectivity", 
                "command": "getCurrentState",
                "params": {},
                "expected_status": "success",
                "timeout": 10,
                "description": "Current state retrieval test"
            },
            
            # Ê©üËÉΩ„ÉÜ„Çπ„Éà
            {
                "id": "func_simple_prompt",
                "category": "functionality",
                "command": "submitPrompt",
                "params": {"prompt": "hello"},
                "expected_status": "success",
                "timeout": 30,
                "description": "Simple prompt submission"
            },
            {
                "id": "func_code_generation",
                "category": "functionality",
                "command": "submitPrompt",
                "params": {"prompt": "write a function that adds two numbers"},
                "expected_status": "success",
                "timeout": 45,
                "description": "Code generation request"
            },
            {
                "id": "func_explanation",
                "category": "functionality",
                "command": "submitPrompt",
                "params": {"prompt": "explain what is a variable in programming"},
                "expected_status": "success",
                "timeout": 45,
                "description": "Explanation request"
            },
            
            # „Ç®„É©„Éº„Éè„É≥„Éâ„É™„É≥„Ç∞„ÉÜ„Çπ„Éà
            {
                "id": "error_unknown_command",
                "category": "error_handling",
                "command": "unknownCommand",
                "params": {},
                "expected_status": "failed",
                "timeout": 10,
                "description": "Unknown command handling"
            },
            {
                "id": "error_empty_prompt",
                "category": "error_handling", 
                "command": "submitPrompt",
                "params": {"prompt": ""},
                "expected_status": "failed",
                "timeout": 20,
                "description": "Empty prompt handling"
            },
            
            # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÜ„Çπ„Éà
            {
                "id": "perf_quick_response",
                "category": "performance",
                "command": "ping",
                "params": {},
                "expected_status": "success",
                "timeout": 5,
                "max_response_time": 3.0,
                "description": "Quick response performance test"
            },
            
            # ‰ø°È†ºÊÄß„ÉÜ„Çπ„Éà
            {
                "id": "reliability_repeated_ping",
                "category": "reliability",
                "command": "ping",
                "params": {},
                "expected_status": "success",
                "timeout": 10,
                "repeat_count": 5,
                "description": "Repeated ping reliability test"
            }
        ]
        
        self.setup_validation_database()
        self.results = []
    
    def setup_validation_database(self):
        """Ê§úË®ºÁî®„Éá„Éº„Çø„Éô„Éº„ÇπÂàùÊúüÂåñ"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS validation_results (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                test_id TEXT NOT NULL,
                category TEXT NOT NULL,
                command TEXT NOT NULL,
                expected_status TEXT NOT NULL,
                actual_status TEXT NOT NULL,
                execution_time REAL NOT NULL,
                success BOOLEAN NOT NULL,
                error_message TEXT,
                response_data TEXT,
                timestamp TEXT NOT NULL
            )
        ''')
        conn.commit()
        conn.close()
        logger.info(f"üìä Validation database initialized: {self.db_path}")
    
    def send_test_request(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """„ÉÜ„Çπ„Éà„É™„ÇØ„Ç®„Çπ„ÉàÈÄÅ‰ø°"""
        request_id = str(uuid.uuid4())
        request = {
            "request_id": request_id,
            "command": test_case["command"],
            "params": test_case.get("params", {})
        }
        
        request_path = f"{self.requests_dir}/{request_id}.json"
        response_path = f"{self.responses_dir}/{request_id}.json"
        
        logger.info(f"üß™ Testing {test_case['id']}: {test_case['description']}")
        
        start_time = time.time()
        
        # „É™„ÇØ„Ç®„Çπ„ÉàÈÄÅ‰ø°
        with open(request_path, 'w') as f:
            json.dump(request, f, indent=2)
        
        # ÂøúÁ≠îÂæÖÊ©ü
        timeout = test_case.get("timeout", 30)
        for i in range(timeout):
            if os.path.exists(response_path):
                with open(response_path, 'r') as f:
                    response = json.load(f)
                
                execution_time = time.time() - start_time
                
                # „ÇØ„É™„Éº„É≥„Ç¢„ÉÉ„Éó
                try:
                    os.remove(response_path)
                except:
                    pass
                
                return {
                    "request_id": request_id,
                    "response": response,
                    "execution_time": execution_time,
                    "timeout": False
                }
            
            time.sleep(1)
        
        # „Çø„Ç§„É†„Ç¢„Ç¶„Éà
        execution_time = time.time() - start_time
        return {
            "request_id": request_id,
            "response": None,
            "execution_time": execution_time,
            "timeout": True
        }
    
    def evaluate_test_result(self, test_case: Dict[str, Any], result: Dict[str, Any]) -> Dict[str, Any]:
        """„ÉÜ„Çπ„ÉàÁµêÊûúË©ï‰æ°"""
        success = True
        error_message = None
        
        if result["timeout"]:
            success = False
            error_message = f"Request timed out after {test_case.get('timeout', 30)}s"
            actual_status = "timeout"
        else:
            response = result["response"]
            actual_status = response.get("final_status", "unknown") if response else "error"
            
            # „Çπ„ÉÜ„Éº„Çø„ÇπÊúüÂæÖÂÄ§„ÉÅ„Çß„ÉÉ„ÇØ
            expected_status = test_case["expected_status"]
            if actual_status != expected_status:
                success = False
                error_message = f"Expected status '{expected_status}' but got '{actual_status}'"
            
            # „Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÉÅ„Çß„ÉÉ„ÇØ
            max_response_time = test_case.get("max_response_time")
            if max_response_time and result["execution_time"] > max_response_time:
                success = False
                error_message = f"Response time {result['execution_time']:.1f}s exceeded limit {max_response_time}s"
        
        return {
            "test_id": test_case["id"],
            "category": test_case["category"],
            "command": test_case["command"],
            "expected_status": test_case["expected_status"],
            "actual_status": actual_status,
            "execution_time": result["execution_time"],
            "success": success,
            "error_message": error_message,
            "response_data": json.dumps(result.get("response")) if result.get("response") else None,
            "description": test_case["description"]
        }
    
    def save_test_result(self, evaluation: Dict[str, Any]):
        """„ÉÜ„Çπ„ÉàÁµêÊûú‰øùÂ≠ò"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO validation_results 
            (test_id, category, command, expected_status, actual_status, 
             execution_time, success, error_message, response_data, timestamp)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            evaluation["test_id"],
            evaluation["category"],
            evaluation["command"],
            evaluation["expected_status"],
            evaluation["actual_status"],
            evaluation["execution_time"],
            evaluation["success"],
            evaluation["error_message"],
            evaluation["response_data"],
            datetime.now().isoformat()
        ))
        
        conn.commit()
        conn.close()
    
    def run_single_test(self, test_case: Dict[str, Any]) -> Dict[str, Any]:
        """Âçò‰∏Ä„ÉÜ„Çπ„ÉàÂÆüË°å"""
        repeat_count = test_case.get("repeat_count", 1)
        
        if repeat_count == 1:
            # Âçò‰∏ÄÂÆüË°å
            result = self.send_test_request(test_case)
            evaluation = self.evaluate_test_result(test_case, result)
            
            # ÁµêÊûúË°®Á§∫
            status_emoji = "‚úÖ" if evaluation["success"] else "‚ùå"
            logger.info(f"   {status_emoji} {evaluation['actual_status']} ({evaluation['execution_time']:.1f}s)")
            if evaluation["error_message"]:
                logger.warning(f"      {evaluation['error_message']}")
            
            return evaluation
        else:
            # Áπ∞„ÇäËøî„ÅóÂÆüË°å
            logger.info(f"   üîÑ Running {repeat_count} iterations...")
            
            all_results = []
            success_count = 0
            total_time = 0
            
            for i in range(repeat_count):
                result = self.send_test_request(test_case)
                evaluation = self.evaluate_test_result(test_case, result)
                
                if evaluation["success"]:
                    success_count += 1
                total_time += evaluation["execution_time"]
                
                all_results.append(evaluation)
                
                # Á∞°ÊΩî„Å™ÈÄ≤ÊçóË°®Á§∫
                status = "‚úÖ" if evaluation["success"] else "‚ùå"
                logger.info(f"      {i+1}/{repeat_count}: {status} ({evaluation['execution_time']:.1f}s)")
            
            # ÈõÜÁ¥ÑÁµêÊûú‰ΩúÊàê
            avg_time = total_time / repeat_count
            success_rate = success_count / repeat_count
            overall_success = success_rate >= 0.8  # 80%‰ª•‰∏ä„ÅßÊàêÂäü„Å®„Åô„Çã
            
            aggregate_evaluation = {
                "test_id": test_case["id"],
                "category": test_case["category"],
                "command": test_case["command"],
                "expected_status": test_case["expected_status"],
                "actual_status": "success" if overall_success else "failed",
                "execution_time": avg_time,
                "success": overall_success,
                "error_message": f"Success rate: {success_rate:.0%}" if not overall_success else None,
                "response_data": f"Iterations: {repeat_count}, Success: {success_count}/{repeat_count}",
                "description": test_case["description"]
            }
            
            logger.info(f"   üìä Overall: {success_count}/{repeat_count} success ({success_rate:.0%}), Avg: {avg_time:.1f}s")
            
            return aggregate_evaluation
    
    def run_comprehensive_validation(self, quick_mode: bool = False):
        """ÂåÖÊã¨ÁöÑÊ§úË®ºÂÆüË°å"""
        logger.info("üß™ === COMPREHENSIVE VALIDATION START ===")
        start_time = datetime.now()
        
        test_cases = self.test_cases
        if quick_mode:
            # „ÇØ„Ç§„ÉÉ„ÇØ„É¢„Éº„Éâ„Åß„ÅØÂü∫Êú¨„ÉÜ„Çπ„Éà„ÅÆ„Åø
            test_cases = [tc for tc in test_cases if tc["category"] in ["connectivity", "functionality"]]
            logger.info("‚ö° Quick mode: Running essential tests only")
        
        # „Ç´„ÉÜ„Ç¥„É™„Åî„Å®„ÅÆÈÄ≤ÊçóÁÆ°ÁêÜ
        category_stats = {cat: {"total": 0, "passed": 0} for cat in self.test_categories.keys()}
        
        for test_case in test_cases:
            category = test_case["category"]
            category_stats[category]["total"] += 1
            
            # „ÉÜ„Çπ„ÉàÂÆüË°å
            evaluation = self.run_single_test(test_case)
            
            if evaluation["success"]:
                category_stats[category]["passed"] += 1
            
            # ÁµêÊûú‰øùÂ≠ò
            self.save_test_result(evaluation)
            self.results.append(evaluation)
        
        # ÁµêÊûú„Çµ„Éû„É™„Éº
        self.print_validation_summary(category_stats, start_time)
    
    def print_validation_summary(self, category_stats: Dict, start_time: datetime):
        """Ê§úË®ºÁµêÊûú„Çµ„Éû„É™„ÉºË°®Á§∫"""
        end_time = datetime.now()
        duration = (end_time - start_time).total_seconds()
        
        total_tests = len(self.results)
        passed_tests = sum(1 for r in self.results if r["success"])
        success_rate = (passed_tests / total_tests * 100) if total_tests > 0 else 0
        
        print("\\n" + "="*60)
        print("üß™ COMPREHENSIVE VALIDATION SUMMARY")
        print("="*60)
        print(f"‚è±Ô∏è  Total Duration: {duration:.1f}s")
        print(f"üìã Total Tests: {total_tests}")
        print(f"‚úÖ Passed: {passed_tests}")
        print(f"‚ùå Failed: {total_tests - passed_tests}")
        print(f"üìä Success Rate: {success_rate:.1f}%")
        
        print("\\nüìà Results by Category:")
        print("-"*40)
        
        for category, description in self.test_categories.items():
            stats = category_stats.get(category, {"total": 0, "passed": 0})
            if stats["total"] > 0:
                cat_rate = (stats["passed"] / stats["total"] * 100)
                status_emoji = "‚úÖ" if cat_rate == 100 else "‚ö†Ô∏è" if cat_rate >= 80 else "‚ùå"
                print(f"{status_emoji} {category.upper()}: {stats['passed']}/{stats['total']} ({cat_rate:.0f}%)")
                print(f"   {description}")
        
        # Â§±Êïó„Åó„Åü„ÉÜ„Çπ„Éà„ÅÆË©≥Á¥∞
        failed_tests = [r for r in self.results if not r["success"]]
        if failed_tests:
            print("\\n‚ùå Failed Tests:")
            print("-"*40)
            for test in failed_tests:
                print(f"‚Ä¢ {test['test_id']}: {test['error_message']}")
        
        # Á∑èÂêàÂà§ÂÆö
        if success_rate >= 95:
            overall_status = "üü¢ EXCELLENT"
        elif success_rate >= 85:
            overall_status = "üü° GOOD"  
        elif success_rate >= 70:
            overall_status = "üü† ACCEPTABLE"
        else:
            overall_status = "üî¥ CRITICAL"
        
        print(f"\\nüèÜ OVERALL SYSTEM STATUS: {overall_status} ({success_rate:.1f}%)")
        print(f"üíæ Detailed results saved to: {self.db_path}")
        print("="*60)
        
        return success_rate >= 70
    
    def generate_detailed_report(self):
        """Ë©≥Á¥∞„É¨„Éù„Éº„ÉàÁîüÊàê"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # „Éá„Éº„Çø„Éô„Éº„Çπ„Åã„ÇâÁµêÊûúÂèñÂæó
        cursor.execute('''
            SELECT test_id, category, command, expected_status, actual_status,
                   execution_time, success, error_message, timestamp
            FROM validation_results 
            ORDER BY timestamp DESC
        ''')
        
        results = cursor.fetchall()
        
        # „É¨„Éù„Éº„ÉàÁîüÊàê
        report_file = f"validation_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        
        with open(report_file, 'w') as f:
            f.write("# Comprehensive Validation Report\\n\\n")
            f.write(f"**Generated:** {datetime.now().isoformat()}\\n")
            f.write(f"**Total Tests:** {len(results)}\\n\\n")
            
            # „Ç´„ÉÜ„Ç¥„É™Âà•„Çª„ÇØ„Ç∑„Éß„É≥
            for category in self.test_categories.keys():
                category_results = [r for r in results if r[1] == category]
                if not category_results:
                    continue
                    
                f.write(f"## {category.upper()}\\n\\n")
                f.write(f"**Description:** {self.test_categories[category]}\\n\\n")
                
                f.write("| Test ID | Command | Expected | Actual | Time | Status |\\n")
                f.write("|---------|---------|----------|--------|------|--------|\\n")
                
                for result in category_results:
                    status_icon = "‚úÖ" if result[6] else "‚ùå"  # success field
                    f.write(f"| {result[0]} | {result[2]} | {result[3]} | {result[4]} | {result[5]:.1f}s | {status_icon} |\\n")
                
                f.write("\\n")
        
        conn.close()
        logger.info(f"üìÑ Detailed report generated: {report_file}")
        return report_file


def main():
    """„É°„Ç§„É≥ÂÆüË°åÈñ¢Êï∞"""
    parser = argparse.ArgumentParser(description='Comprehensive System Validation')
    parser.add_argument('--quick', action='store_true', help='Run only essential tests')
    parser.add_argument('--report-only', action='store_true', help='Generate report from existing results')
    
    args = parser.parse_args()
    
    validator = ComprehensiveValidator()
    
    if args.report_only:
        validator.generate_detailed_report()
    else:
        success = validator.run_comprehensive_validation(args.quick)
        validator.generate_detailed_report()
        
        # ÁµÇ‰∫Ü„Ç≥„Éº„Éâ
        exit_code = 0 if success else 1
        exit(exit_code)


if __name__ == "__main__":
    main()