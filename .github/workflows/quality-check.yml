name: Code Quality and Performance

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  schedule:
    # Run quality checks daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'

jobs:
  code-quality:
    name: Code Quality Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install quality tools
      run: |
        python -m pip install --upgrade pip
        pip install black isort pylint mypy bandit safety
        pip install -r requirements.txt
    
    - name: Check code formatting with Black
      run: |
        black --check --diff .
    
    - name: Check import sorting with isort
      run: |
        isort --check-only --diff .
    
    - name: Lint with pylint
      run: |
        pylint evaluate_agents.py tests/ --disable=C0114,C0115,C0116 --exit-zero
    
    - name: Type checking with mypy
      run: |
        mypy evaluate_agents.py --ignore-missing-imports --no-strict-optional
    
    - name: Security scan with bandit
      run: |
        bandit -r . -x tests/ -f json -o bandit-results.json
    
    - name: Dependency security scan
      run: |
        safety check --json --output safety-results.json || true
    
    - name: Upload quality reports
      uses: actions/upload-artifact@v3
      with:
        name: quality-reports-${{ github.sha }}
        path: |
          bandit-results.json
          safety-results.json
        retention-days: 30

  performance-test:
    name: Performance Testing
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-benchmark memory-profiler
    
    - name: Run performance benchmarks
      run: |
        python -c "
        import time
        import json
        import evaluate_agents
        
        # Performance test for evaluation system
        print('ðŸƒ Running performance tests...')
        
        config = {
            'demo_mode': True,
            'instructions_file': 'instructions.json',
            'results_dir': 'perf_results',
            'timeout': 60,
            'max_retries': 3,
            'retry_delay': 1
        }
        
        start_time = time.time()
        evaluator = evaluate_agents.AgentEvaluator(config)
        init_time = time.time() - start_time
        
        # Test evaluation performance
        start_time = time.time()
        evaluator.run_evaluation()
        eval_time = time.time() - start_time
        
        # Test report generation performance
        start_time = time.time()
        evaluator.generate_report()
        report_time = time.time() - start_time
        
        # Save performance metrics
        perf_data = {
            'initialization_time': init_time,
            'evaluation_time': eval_time,
            'report_generation_time': report_time,
            'total_time': init_time + eval_time + report_time
        }
        
        with open('performance_metrics.json', 'w') as f:
            json.dump(perf_data, f, indent=2)
        
        print(f'â±ï¸ Performance Results:')
        print(f'   Initialization: {init_time:.3f}s')
        print(f'   Evaluation: {eval_time:.3f}s')
        print(f'   Report Generation: {report_time:.3f}s')
        print(f'   Total: {perf_data[\"total_time\"]:.3f}s')
        
        # Performance thresholds
        if perf_data['total_time'] > 60:
            print('âŒ Performance test failed: Total time exceeds 60 seconds')
            exit(1)
        else:
            print('âœ… Performance test passed')
        "
    
    - name: Upload performance results
      uses: actions/upload-artifact@v3
      with:
        name: performance-metrics-${{ github.sha }}
        path: performance_metrics.json
        retention-days: 30

  documentation-check:
    name: Documentation Validation
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Check documentation completeness
      run: |
        echo "ðŸ“ Checking documentation completeness..."
        
        # Required documentation files
        required_files=(
          "README.md"
          "docs/TASKS.md"
          "requirements.txt"
          ".github/workflows/ci.yml"
        )
        
        missing_files=()
        for file in "${required_files[@]}"; do
          if [[ ! -f "$file" ]]; then
            missing_files+=("$file")
          fi
        done
        
        if [[ ${#missing_files[@]} -gt 0 ]]; then
          echo "âŒ Missing required documentation files:"
          printf '   - %s\n' "${missing_files[@]}"
          exit 1
        fi
        
        echo "âœ… All required documentation files present"
    
    - name: Validate README structure
      run: |
        echo "ðŸ“– Validating README structure..."
        
        required_sections=(
          "# "
          "## "
          "### "
        )
        
        for section in "${required_sections[@]}"; do
          if ! grep -q "^$section" README.md; then
            echo "âŒ README.md missing required section starting with '$section'"
            exit 1
          fi
        done
        
        echo "âœ… README.md structure validated"
    
    - name: Check for TODO items
      run: |
        echo "ðŸ” Checking for TODO items..."
        
        todo_count=$(grep -r "TODO\|FIXME\|HACK" --include="*.py" --include="*.md" . | wc -l)
        
        if [[ $todo_count -gt 10 ]]; then
          echo "âš ï¸ Warning: $todo_count TODO/FIXME/HACK items found"
          grep -r "TODO\|FIXME\|HACK" --include="*.py" --include="*.md" . | head -10
        else
          echo "âœ… Acceptable number of TODO items: $todo_count"
        fi

  coverage-report:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov coverage
    
    - name: Run tests with coverage
      run: |
        pytest tests/ --cov=. --cov-report=html --cov-report=term --cov-report=json
    
    - name: Coverage summary
      run: |
        coverage report --show-missing
        
        # Extract coverage percentage
        coverage_percent=$(coverage report | grep TOTAL | awk '{print $4}' | sed 's/%//')
        echo "COVERAGE_PERCENT=$coverage_percent" >> $GITHUB_ENV
        
        # Set coverage threshold
        threshold=70
        
        if (( $(echo "$coverage_percent < $threshold" | bc -l) )); then
          echo "âŒ Coverage $coverage_percent% is below threshold $threshold%"
          exit 1
        else
          echo "âœ… Coverage $coverage_percent% meets threshold $threshold%"
        fi
    
    - name: Upload coverage reports
      uses: actions/upload-artifact@v3
      with:
        name: coverage-reports-${{ github.sha }}
        path: |
          htmlcov/
          coverage.json
        retention-days: 30