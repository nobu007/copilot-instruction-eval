#!/usr/bin/env python3
"""
File-based Evaluation System Test Script

ãƒ•ã‚¡ã‚¤ãƒ«ãƒ™ãƒ¼ã‚¹è©•ä¾¡ã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import time
from file_based_evaluation_client import FileBasedEvaluationClient, TestCase

def main():
    print("ğŸš€ File-based Copilot Evaluation System Test")
    print("=" * 50)
    
    # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
    client = FileBasedEvaluationClient()
    
    # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ç›£è¦–é–‹å§‹
    client.start_monitoring()
    
    try:
        # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        status = client.get_status()
        print(f"ğŸ“Š System Status:")
        print(f"  Base directory: {status['base_directory']}")
        print(f"  Monitoring active: {status['monitoring_active']}")
        print(f"  Directories created: âœ…")
        
        # ç°¡å˜ãªãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹
        test_cases = [
            TestCase(
                test_id="simple_test_001",
                prompt="Write a simple Python hello world function",
                category="basic_code",
                expected_elements=["def", "hello", "print"],
                description="Simple hello world function test"
            ),
            TestCase(
                test_id="simple_test_002", 
                prompt="Create a function that adds two numbers",
                category="basic_code",
                expected_elements=["def", "add", "return"],
                description="Simple addition function test"
            )
        ]
        
        print(f"\nğŸ§ª Running {len(test_cases)} test cases...")
        
        # è©•ä¾¡å®Ÿè¡Œ
        results = client.run_batch_evaluation(
            test_cases=test_cases,
            models=["copilot/gpt-4"],  # 1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
            modes=["agent"]           # 1ã¤ã®ãƒ¢ãƒ¼ãƒ‰ã§ãƒ†ã‚¹ãƒˆ
        )
        
        # çµæœè¡¨ç¤º
        print(f"\nğŸ“Š Test Results:")
        print(f"Total tests: {results['summary']['total_tests']}")
        print(f"Successful tests: {results['summary']['successful_tests']}")
        print(f"Success rate: {results['summary']['success_rate']:.2%}")
        
        if results['results']:
            print(f"\nğŸ“ Individual Results:")
            for result in results['results']:
                status_icon = "âœ…" if result['success'] else "âŒ"
                print(f"  {status_icon} {result['test_id']}: {result['execution_time']:.2f}s")
                if result['error_message']:
                    print(f"    Error: {result['error_message']}")
                else:
                    print(f"    Response preview: {result['response_preview'][:100]}...")
        
        print(f"\nğŸ’¾ Results saved to: {client.results_dir}")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸ Test interrupted by user")
    except Exception as e:
        print(f"\nâŒ Test failed: {e}")
    finally:
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        client.stop_monitoring()
        print(f"\nğŸ›‘ Test completed")

if __name__ == "__main__":
    main()
